{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Import the required Python dependencies\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import LongType, DoubleType, IntegerType, StringType, BooleanType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import Tokenizer as NLPTokenizer\n",
    "from sparknlp.annotator import Stemmer, Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Instantiate a Spark Context\n",
    "conf = SparkConf().setMaster(\"local\").set(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.2.1\").setAppName(\"Natural Language Processing - Sentiment Analysis\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) Load the labelled Airline Tweet Corpus\n",
    "schema = StructType([\n",
    "    StructField(\"unit_id\", LongType()), \n",
    "    StructField(\"golden\", BooleanType()), \n",
    "    StructField(\"unit_state\", StringType()), \n",
    "    StructField(\"trusted_judgments\", IntegerType()), \n",
    "    StructField(\"last_judgment_at\", StringType()), \n",
    "    StructField(\"airline_sentiment\", StringType()), \n",
    "    StructField(\"airline_sentiment_confidence\", DoubleType()), \n",
    "    StructField(\"negative_reason\", StringType()), \n",
    "    StructField(\"negative_reason_confidence\", DoubleType()), \n",
    "    StructField(\"airline\", StringType()), \n",
    "    StructField(\"airline_sentiment_gold\", StringType()), \n",
    "    StructField(\"name\", StringType()), \n",
    "    StructField(\"negative_reason_gold\", StringType()), \n",
    "    StructField(\"retweet_count\", IntegerType()), \n",
    "    StructField(\"text\", StringType()), \n",
    "    StructField(\"tweet_coordinates\", StringType()), \n",
    "    StructField(\"tweet_created\", StringType()), \n",
    "    StructField(\"tweet_id\", StringType()), \n",
    "    StructField(\"tweet_location\", StringType()), \n",
    "    StructField(\"user_timezone\", StringType())\n",
    "])\n",
    "\n",
    "airline_tweets_df = sqlContext.read.format('com.databricks.spark.csv').schema(schema) \\\n",
    "    .options(header = 'true', inferschema = 'false') \\\n",
    "    .load('./data/twitter-data/airline-tweets-labelled-corpus.csv')\n",
    "airline_tweets_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) Since we are only interested in detecting tweets with negative sentiment, generate a new label \n",
    "# whereby if the sentiment is negative, the label is TRUE (Positive Outcome) otherwise FALSE\n",
    "airline_tweets_with_labels_df = airline_tweets_df.withColumn(\"negative_sentiment_label\", \n",
    "    when(col(\"airline_sentiment\") == \"negative\", lit(\"true\")).otherwise(lit(\"false\"))) \\\n",
    "    .select(\"unit_id\", \"text\", \"negative_sentiment_label\")\n",
    "airline_tweets_with_labels_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5) Pre-Process the tweets using the Feature Transformers NATIVE to Spark MLlib\n",
    "# (5.1) Remove any tweets with null textual content\n",
    "# (5.2) Tokenize the textual content using the Tokenizer Feature Transformer\n",
    "# (5.3) Remove Stop Words from the sequence of tokens using the StopWordsRemover Feature Transformer\n",
    "# (5.4) Concatenate the filtered sequence of tokens into a single string for 3rd party pre-processing (-> spark-nlp)\n",
    "\n",
    "filtered_df = airline_tweets_with_labels_df.filter(\"text is not null\")\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens_1\")\n",
    "tokenized_df = tokenizer.transform(filtered_df)\n",
    "remover = StopWordsRemover(inputCol=\"tokens_1\", outputCol=\"filtered_tokens\")\n",
    "preprocessed_part_1_df = remover.transform(tokenized_df)\n",
    "preprocessed_part_1_df = preprocessed_part_1_df.withColumn(\"concatenated_filtered_tokens\", \n",
    "    concat_ws(\" \", col(\"filtered_tokens\")))\n",
    "preprocessed_part_1_df.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (6) Define a NLP pipeline to pre-process the tweets using the spark-nlp 3rd party library\n",
    "# (6.1) Annotate the string containing the concatenated filtered tokens using the DocumentAssembler Transformer\n",
    "# (6.2) Re-tokenize the document using the Tokenizer Annotator\n",
    "# (6.3) Apply Stemming to the Tokens using the Stemmer Annotator\n",
    "# (6.4) Clean and lowercase all the Tokens using the Normalizer Annotator\n",
    "\n",
    "document_assembler = DocumentAssembler().setInputCol(\"concatenated_filtered_tokens\")\n",
    "tokenizer = NLPTokenizer().setInputCols([\"document\"]).setOutputCol(\"tokens_2\")\n",
    "stemmer = Stemmer().setInputCols([\"tokens_2\"]).setOutputCol(\"stems\")\n",
    "normalizer = Normalizer().setInputCols([\"stems\"]).setOutputCol(\"normalised_stems\")\n",
    "pipeline = Pipeline(stages=[document_assembler, tokenizer, stemmer, normalizer])\n",
    "pipeline_model = pipeline.fit(preprocessed_part_1_df)\n",
    "preprocessed_df = pipeline_model.transform(preprocessed_part_1_df)\n",
    "preprocessed_df.select(\"unit_id\", \"text\", \"negative_sentiment_label\", \"normalised_stems\").show(3, False)\n",
    "preprocessed_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (7) We could proceed to use the 3rd party annotators available in spark-nlp to train a sentiment model, such as\n",
    "# SentimentDetector and ViveknSentimentDetector respectively\n",
    "# However in this case study, we will use the Feature Extractors native to Spark MLlib to generate feature vectors \n",
    "# to train our subsequent machine learning model. In this case, we will use MLlib's TF-IDF Feature Extractor.\n",
    "\n",
    "# (7.1) Extract the normalised stems from the spark-nlp Annotator Array Structure\n",
    "exploded_df = preprocessed_df.withColumn(\"stems\", explode(\"normalised_stems\")) \\\n",
    "    .withColumn(\"stems\", col(\"stems\").getItem(\"result\")) \\\n",
    "    .select(\"unit_id\", \"negative_sentiment_label\", \"text\", \"stems\")\n",
    "exploded_df.show(10, False)\n",
    "\n",
    "# (7.2) Group by Unit ID and aggregate then normalised stems into a sequence of tokens\n",
    "aggregated_df = exploded_df.groupBy(\"unit_id\").agg(concat_ws(\" \", collect_list(col(\"stems\"))), \n",
    "    first(\"text\"), first(\"negative_sentiment_label\")) \\\n",
    "    .toDF(\"unit_id\", \"tokens\", \"text\", \"negative_sentiment_label\") \\\n",
    "    .withColumn(\"tokens\", split(col(\"tokens\"), \" \").cast(\"array<string>\"))\n",
    "aggregated_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (8) Generate Term Frequency Feature Vectors by passing the sequence of tokens to the HashingTF Transformer.\n",
    "# Then fit an IDF Estimator to the Featurized Dataset to generate the IDFModel.\n",
    "# Finally pass the TF Feature Vectors to the IDFModel to scale based on frequency across the corpus\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"tokens\", outputCol=\"raw_features\", numFeatures=280)\n",
    "features_df = hashingTF.transform(aggregated_df)\n",
    "\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idf_model = idf.fit(features_df)\n",
    "scaled_features_df = idf_model.transform(features_df)\n",
    "scaled_features_df.cache()\n",
    "scaled_features_df.show(8, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (9) Index the label column using StringIndexer\n",
    "# Now a label of 1.0 = FALSE (Not Negative Sentiment) and a label of 0.0 = TRUE (Negative Sentiment)\n",
    "indexer = StringIndexer(inputCol = \"negative_sentiment_label\", outputCol = \"label\").fit(scaled_features_df)\n",
    "scaled_features_indexed_label_df = indexer.transform(scaled_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (10) Split the index-labelled Scaled Feature Vectors into Training and Test DataFrames\n",
    "train_df, test_df = scaled_features_indexed_label_df.randomSplit([0.9, 0.1], seed=12345)\n",
    "train_df.count(), test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (11) Train a Classification Tree Model on the Training DataFrame\n",
    "decision_tree = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label')\n",
    "decision_tree_model = decision_tree.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (12) Apply the Trained Classification Tree Model to the Test DataFrame to make predictions\n",
    "test_decision_tree_predictions_df = decision_tree_model.transform(test_df)\n",
    "print(\"TEST DATASET PREDICTIONS AGAINST ACTUAL LABEL: \")\n",
    "test_decision_tree_predictions_df.select(\"prediction\", \"label\", \"text\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (13) Compute the Confusion Matrix for our Decision Tree Classifier on the Test DataFrame\n",
    "predictions_and_label = test_decision_tree_predictions_df.select(\"prediction\", \"label\").rdd\n",
    "metrics = MulticlassMetrics(predictions_and_label)\n",
    "print(\"N = %g\" % test_decision_tree_predictions_df.count())\n",
    "print(metrics.confusionMatrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (14) For completeness let us train a Decision Tree Classifier using Feature Vectors derived from the Bag of Words algorithm\n",
    "# Note that we have already computed these Feature Vectors when applying the HashingTF Transformer in Cell #8 above\n",
    "\n",
    "# (14.1) Create Training and Test DataFrames based on the Bag of Words Feature Vectors\n",
    "bow_indexer = StringIndexer(inputCol = \"negative_sentiment_label\", outputCol = \"label\").fit(features_df)\n",
    "bow_features_indexed_label_df = bow_indexer.transform(features_df).withColumnRenamed(\"raw_features\", \"features\")\n",
    "bow_train_df, bow_test_df = bow_features_indexed_label_df.randomSplit([0.9, 0.1], seed=12345)\n",
    "\n",
    "# (14.2) Train a Decision Tree Classifier using the Bag of Words Feature Vectors\n",
    "bow_decision_tree = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label')\n",
    "bow_decision_tree_model = bow_decision_tree.fit(bow_train_df)\n",
    "\n",
    "# (14.3) Apply the Bag of Words Decision Tree Classifier to the Test DataFrame and generate the Confusion Matrix\n",
    "bow_test_decision_tree_predictions_df = bow_decision_tree_model.transform(bow_test_df)\n",
    "bow_predictions_and_label = bow_test_decision_tree_predictions_df.select(\"prediction\", \"label\").rdd\n",
    "bow_metrics = MulticlassMetrics(bow_predictions_and_label)\n",
    "print(\"N = %g\" % bow_test_decision_tree_predictions_df.count())\n",
    "print(bow_metrics.confusionMatrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (15) Persist the trained Decision Tree Classifier to disk for later use\n",
    "bow_decision_tree_model.save('./models/airline-sentiment-analysis-decision-tree-classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (16) Stop the Spark Context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
